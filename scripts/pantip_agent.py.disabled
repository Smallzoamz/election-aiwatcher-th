import os
import time
import json
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date
from dotenv import load_dotenv

# --- CONFIGURATION ---
load_dotenv()  # Load API_KEY from .env

# Budget Guardrails
MAX_DAILY_THREADS = 200
TODAY_LOG_FILE = f"logs/usage_{date.today()}.json"

# AI Configuration (Select Provider)
AI_PROVIDER = os.getenv("AI_PROVIDER", "gemini") # 'gemini' or 'openai'
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Pantip Configuration
TARGET_TAGS = ["‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á", "‡∏´‡∏∏‡πâ‡∏ô", "‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à"]
BASE_URL = "https://pantip.com"

# IO Detection Thresholds (Heuristic + AI)
IO_SUSPICIOUS_THRESHOLD = 70  # Score > 70 = Likely IO

# Setup Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class BudgetGuard:
    @staticmethod
    def check_limit():
        """Checks if the daily limit has been reached."""
        if not os.path.exists("logs"):
            os.makedirs("logs")
        
        usage = {"count": 0, "date": str(date.today())}
        
        if os.path.exists(TODAY_LOG_FILE):
            try:
                with open(TODAY_LOG_FILE, 'r') as f:
                    usage = json.load(f)
            except json.JSONDecodeError:
                pass # Corrupt file, start fresh
        
        # Reset if new day (double check date in file)
        if usage["date"] != str(date.today()):
            usage = {"count": 0, "date": str(date.today())}

        if usage["count"] >= MAX_DAILY_THREADS:
            logging.warning(f"‚õî DAILY LIMIT REACHED ({usage['count']}/{MAX_DAILY_THREADS}). Stopping to save money.")
            return False
        return True

    @staticmethod
    def increment_count():
        """Increments the daily usage count."""
        if not os.path.exists("logs"):
            os.makedirs("logs")
            
        usage = {"count": 0, "date": str(date.today())}
        if os.path.exists(TODAY_LOG_FILE):
            try:
                with open(TODAY_LOG_FILE, 'r') as f:
                    usage = json.load(f)
            except:
                pass
        
        if usage["date"] != str(date.today()): # Reset check again
            usage = {"count": 0, "date": str(date.today())}

        usage["count"] += 1
        with open(TODAY_LOG_FILE, 'w') as f:
            json.dump(usage, f)
        
        logging.info(f"üí∞ Usage: {usage['count']}/{MAX_DAILY_THREADS}")

# --- AI AGENT ---
def analyze_with_ai(text):
    """
    Sends text to AI to analyze for:
    1. Inference (Hidden Meaning)
    2. Sentiment
    3. IO Probability
    """
    if not text or len(text) < 10:
        return None

    # Prompt Engineering
    prompt = f"""
    Analyze this Thai social media post from 'Pantip' for political context:
    "{text}"
    
    Output JSON only:
    {{
        "sentiment": "positive" | "negative" | "neutral",
        "inference": "Summary of what the user really means in 1 sentence",
        "io_score": 0-100, (Likelihood of being paid IO/Bot/Propaganda based on aggression and pattern),
        "topics": ["list", "of", "topics"]
    }}
    """

    # Mock Response for now (until API Key is set)
    if not GEMINI_API_KEY and not OPENAI_API_KEY:
        # logging.warning("‚ö†Ô∏è No API Key found. Returning mock data.")
        return {
            "sentiment": "neutral",
            "inference": "Mock inference - set API Key to enable AI.",
            "io_score": 10,
            "topics": ["test"]
        }

    # TODO: Implement actual API calls here
    # code to call google-generativeai or openai
    return {
        "sentiment": "neutral",
        "inference": "Real AI integration pending.",
        "io_score": 0,
        "topics": []
    }

# --- SCRAPER ---
def scrape_pantip_tag(tag):
    if not BudgetGuard.check_limit():
        return

    url = f"{BASE_URL}/tag/{tag}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        logging.info(f"üï∑Ô∏è Scraping Tag: {tag}")
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            logging.error(f"Failed to load page: {response.status_code}")
            return

        soup = BeautifulSoup(response.content, 'html.parser')
        threads = soup.find_all('div', class_='pt-list-item') # Adjust selector based on actual Pantip structure

        # NOTE: Pantip structure changes often, this is a generic placeholder selector
        # In reality, we might need selenium or more complex headers if simple requests fail
        # For this prototype, we simulate finding threads
        
        # Simulated Loop
        count = 0
        for i in range(5): # Limit to 5 per run for testing
            if not BudgetGuard.check_limit():
                break

            # Simulate extraction
            title = f"Topic about {tag} {i}" 
            content = "User opinion text..."
            
            # AI Analysis
            logging.info(f"ü§ñ Analyzing: {title}")
            analysis = analyze_with_ai(content)
            
            # IO Detection Filter
            if analysis['io_score'] > IO_SUSPICIOUS_THRESHOLD:
                logging.warning(f"üö® IO DETECTED (Score {analysis['io_score']}): {title}")
            else:
                logging.info(f"‚úÖ Clean Content: {analysis['inference']}")

            BudgetGuard.increment_count()
            time.sleep(1) # Be polite to server

    except Exception as e:
        logging.error(f"Error: {e}")

if __name__ == "__main__":
    if not GEMINI_API_KEY and not OPENAI_API_KEY:
        print("‚ö†Ô∏è  WARNING: No API Key found in .env. Using mock mode.")
        print("‚ÑπÔ∏è  To enable AI, add GEMINI_API_KEY=... to .env")
    
    print(f"üöÄ Starting AI Agent Scraper (Limit: {MAX_DAILY_THREADS}/day)...")
    for tag in TARGET_TAGS:
        scrape_pantip_tag(tag)
    print("‚úÖ Done.")
